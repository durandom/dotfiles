# see https://github.com/sigoden/aichat/blob/main/config.example.yaml

# model: openai:gpt-4
# ---- clients ----
clients:
  # All clients have the following configuration:
  # - type: xxxx
  #   name: xxxx                                      # Only use it to distinguish clients with the same client type. Optional
  #   models:
  #     - name: xxxx                                  # Chat model
  #       max_input_tokens: 100000
  #       supports_vision: true
  #       supports_function_calling: true
  #     - name: xxxx                                  # Embedding model
  #       type: embedding
  #       default_chunk_size: 1500
  #       max_batch_size: 100
  #     - name: xxxx                                  # Reranker model
  #       type: reranker
  #   patch:                                          # Patch api
  #     chat_completions:                             # Api type, possible values: chat_completions, embeddings, and rerank
  #       <regex>:                                    # The regex to match model names, e.g. '.*' 'gpt-4o' 'gpt-4o|gpt-4-.*'
  #         url: ''                                   # Patch request url
  #         body:                                     # Patch request body
  #           <json>
  #         headers:                                  # Patch request headers
  #           <key>: <value>
  #   extra:
  #     proxy: socks5://127.0.0.1:1080                # Set proxy
  #     connect_timeout: 10                           # Set timeout in seconds for connect to api

  # See https://platform.openai.com/docs/quickstart
  # - type: openai
  #   api_base: https://api.openai.com/v1               # Optional
  #   api_key: xxx
  #   organization_id: org-xxx                          # Optional

  # For any platform compatible with OpenAI's API
  - type: openai-compatible
    name: granite
    api_base: https://granite-3-2-8b-instruct--apicast-production.apps.int.stc.ai.prod.us-east-1.aws.paas.redhat.com/v1
    # api_key:

    models:
      - name: /data/granite-3.2-8b-instruct
        max_input_tokens: 131072

  # ollama pull nomic-embed-text
  #

  - type: openai-compatible
    name: ollama
    api_base: http://localhost:11434/v1
    models:
      - name: nomic-embed-text
        type: embedding
        default_chunk_size: 1000
        max_batch_size: 50
      # - name: deepseek-r1
      #   max_input_tokens: 131072
      # - name: llama3.1
      #   max_input_tokens: 128000
      #   supports_function_calling: true
      # - name: llama3.2-vision
      #   max_input_tokens: 131072
      #   supports_vision: true
  - type: openai-compatible
    name: ollama
    api_base: http://localhost:11434/v1
    models:
      - name: nomic-embed-text
        type: embedding
        default_chunk_size: 1000
        max_batch_size: 50
      # - name: deepseek-r1
      #   max_input_tokens: 131072
      # - name: llama3.1
      #   max_input_tokens: 128000
      #   supports_function_calling: true
      # - name: llama3.2-vision
      #   max_input_tokens: 131072
      #   supports_vision: true

  # inceptionlabs
  - type: openai-compatible
    name: inception
    api_base: https://api.inceptionlabs.ai/v1
    # api_key:
    models:
      - name: mercury-coder-small
        max_input_tokens: 16384

  # WCA
  - type: openai-compatible
    name: wca
    api_base: http://localhost:5001/v1
    api_key: dummy
    models:
      - name: watson-ai
        max_input_tokens: 16384

editor: code

# ---- RAG ----
# See [RAG-Guide](https://github.com/sigoden/aichat/wiki/RAG-Guide) for more details.
rag_embedding_model: ollama:nomic-embed-text        # Specifies the embedding model used for context retrieval
# rag_reranker_model: null         # Specifies the reranker model used for sorting retrieved documents
# rag_top_k: 5                     # Specifies the number of documents to retrieve for answering queries
# rag_chunk_size: null             # Defines the size of chunks for document processing in characters
# rag_chunk_overlap: null          # Defines the overlap between chunks
# Defines the query structure using variables like __CONTEXT__ and __INPUT__ to tailor searches to specific needs
# rag_template: |
#   Answer the query based on the context while respecting the rules. (user query, some textual context and rules, all inside xml tags)

#   <context>
#   __CONTEXT__
#   </context>

#   <rules>
#   - If you don't know, just say so.
#   - If you are not sure, ask for clarification.
#   - Answer in the same language as the user query.
#   - If the context appears unreadable or of poor quality, tell the user then answer as best as you can.
#   - If the answer is not in the context but you think you know the answer, explain that to the user then answer with your own knowledge.
#   - Answer directly and without using xml tags.
#   </rules>

#   <user_query>
#   __INPUT__
#   </user_query>

# # Define document loaders to control how RAG and `.file`/`--file` load files of specific formats.
# document_loaders:
#   # You can add custom loaders using the following syntax:
#   #   <file-extension>: <command-to-load-the-file>
#   # Note: Use `$1` for input file and `$2` for output file. If `$2` is omitted, use stdout as output.
#   pdf: 'pdftotext $1 -'                         # Load .pdf file, see https://poppler.freedesktop.org to set up pdftotext
#   docx: 'pandoc --to plain $1'                  # Load .docx file, see https://pandoc.org to set up pandoc
